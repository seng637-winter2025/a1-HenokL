>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group 24      |
|-----------------|
| Henok                |   
| Falmata              |   
| X               |   
| X              |   


**Table of Contents**


[1 Introduction](#introduction)

[2 High-Level Testing Plan](<#high-level-testing-plan>)

[3 Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was divided](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction

In this lab, we evaluated an ATM simulation system using three testing strategies: **exploratory testing**, **manual functional testing**, and **regression testing**. Our goals were to:  
- Uncover defects in the system.  
- Compare the effectiveness of different testing methods.  
- Practice using Jira for defect tracking. 

### What We Knew Before the Lab:
Before starting, we had a basic understanding of:

- Exploratory Testing: A flexible, ad-hoc approach where software testers learn the system on-the-fly and design tests dynamically. We imagined it would feel like "detective work" to find hidden bugs.

- Manual Functional Testing: A methodical process using predefined test cases. We assumed it would feel more systematic but less creative.

- Regression Testing: A process to ensure that new changes (e.g., bug fixes) don’t break existing functionality. We thought of it as a "safety net" for the system. 

# High-Level Testing Plan


We simulated real-world user behavior to uncover unexpected flaws. Key focus areas included:  
1. **Transactions**:  
   - Deposits, withdrawals, and transfers.  
   - Example: Transferring $0.00 deducted $0.50 from Savings in V1.0.  
2. **Edge Cases**:  
   - Testing extreme inputs (e.g., 23-digit PINs, $0.00 transfers).  
   - Example: A 23-digit PIN crashed the system with a garbled green screen.  
3. **UI/UX**:  
   - Checking menus, error messages, and receipts.  
   - Example: The prompt asked, "**Wood** you like another transaction?"  

### Risk-Prone Features  
We prioritized high-impact features:  
- Balance calculations (e.g., depositing $100 *subtracted* $10 in V1.0).  
- Fund transfers (e.g., receipts reversed transfer directions).  


# Comparison of exploratory and manual functional testing

| **Aspect**               | **Exploratory Testing**                                  | **Manual Functional Testing**                |  
|--------------------------|---------------------------------------------------------|----------------------------------------------|  
| **Mindset**              | “What happens if I…?” – Curiosity-driven, like solving a puzzle. | “Does it do what it’s supposed to?” – Verifying a checklist. |  
| **Bug Hunting**          | Found hidden flaws (e.g., transferring $0.00 *subtracted* money!). | Confirmed basics (e.g., preset withdrawals dispensed cash). |  
| **Creativity**           | Led to oddball tests (e.g., 23-digit PIN crashes the system). | Followed scripts (e.g., testing $20 withdrawals). |  
| **Surprises**            | Exposed UI quirks (e.g., Money Market balance showing “Unknown Error”). | Missed edge cases (e.g., missing Savings account option). |  
| **Speed vs. Precision**  | Fast for uncovering critical issues (e.g., balance deductions). | Slower but ensured no step was overlooked. |  
| **Documentation**        | Notes were messy but real-time (e.g., “Receipt flips transfer direction!”). | Neat logs (e.g., “Step 3: Withdraw $60 – PASS”). |  
| **User Experience**      | Mimicked confused users (e.g., entering gibberish PINs). | Mimicked ideal users (e.g., valid transactions only). |  
| **Team Dynamics**        | Felt collaborative – “Hey, try transferring $0.00!” | Felt structured – “You test Option 3; I’ll test Option 4.” |  

#### Why Both Matter:  
- **Exploratory testing** was our “aha!” moment – like realizing the ATM charged a secret $0.50 fee for *doing nothing*.  
- **Manual testing** was our safety net – ensuring the basics (e.g., withdrawing $20) didn’t break.  
- Together, they painted a full picture: the ATM worked… until it *didn’t*. 

During an Exploratory testing – we tried weird stuff just to see what would break. Like, who knew transferring $0.00 would *steal* money from your account? Manual testing was more like following predefined steps like: “Step 1: Put in card. Step 2: Take out $20.” It worked, but it didn’t catch the sneaky bugs.  

Together, they were like a superhero duo. Exploratory testing found the “Why is my balance $500 instead of $5000?!” mess. Manual testing made sure the basics (like withdrawing cash) didn’t blow up. Not perfect, but at least we knew the ATM could handle real people.   

# Notes and discussion of the peer reviews of defect reports

Peer reviews improved our defect reports by:  
- **Adding Clarity**:  
  - Initially missing screenshots (e.g., "Unknown Error" screen).  
  - Added attachments to show bugs like "$500 instead of $5000."  
- **Prioritizing Severity**:  
  - Critical bugs (e.g., balance deductions) ranked higher than cosmetic issues.  
- **Refining Reproducibility**:  
  - Simplified steps for complex bugs (e.g., "$0.00 transfer issue").  

These reviews taught us to write **developer-friendly reports** – clear, concise, and actionable. 

# How the pair testing was managed and team work/effort was divided 

We divided tasks based on expertise and rotated roles:
- **Henok**: Focused on transactions (deposits, transfers) and defect documentation.  
- **Falmata**: Tested UI/UX (balance inquiries, error handling) and managed Jira.  

### Pair Testing Strategy  
- **Driver-Reporter Model**:  
  - **Driver**: Executed tests (e.g., withdrawing $60).  
  - **Reporter**: Took notes and suggested edge cases (e.g., "What if we try $0?").  

### Collaboration  
- Daily sync-ups ensured alignment.  
- Example: When Henok found the "Wood" typo, Falmata cross-checked it in both versions.  


# Difficulties encountered, challenges overcome, and lessons learned

### Challenges  
1. **Divergent Git Branches**:  
   - *Problem*: Code push failures due to conflicting versions.  
   - *Solution*: Resolved with `git rebase` after teamwork and research.  
2. **Inconsistent Bugs**:  
   - *Problem*: Bugs behaved differently in V1.0 vs. V1.1 (e.g., $10 vs. $1 deduction).  
   - *Solution*: Tagged defects with versions and retested fixes.  
3. **Time Constraints**:  
   - Regression testing was time-consuming, especially when multiple bugs were fixed.  
   - Bugs behaved differently in V1.0 and V1.1, requiring separate regression tests for each version.    

### Lessons Learned  
- **Test Early**: Catching bugs early saved time (e.g., $0.50 fee).  
- **Document Everything**: Clear steps helped developers replicate issues.  
- **Communicate**: Daily check-ins prevented duplicated efforts.  



# Comments/feedback on the lab and lab document itself

The lab was a valuable introduction to software testing, teaching us to think critically and collaborate effectively – even if the ATM seemed determined to fight us! 