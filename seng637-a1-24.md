>   **SENG 637 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: Group 24      |
|-----------------|
| Henok                |   
| Falmata              |   
| X               |   
| X              |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1. Introduction](#introduction)

[2. High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3. Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[4. Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5. How the pair testing was managed and team work/effort was divided](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6. Difficulties encountered, challenges overcome, and lessons learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7. Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

1 # Introduction

In this lab, we evaluated an ATM simulation system using two methods: exploratory testing (unscripted, curiosity-driven testing) and manual functional testing (structured, scripted test cases). Our goal was to uncover defects, compare testing strategies, and practice using tools like Jira for defect tracking.

### What We Knew Before the Lab:
Before starting, we had a basic understanding of:

- Exploratory Testing: A flexible, ad-hoc approach where software testers learn the system on-the-fly and design tests dynamically. We imagined it would feel like "detective work" to find hidden bugs.

- Manual Functional Testing: A methodical process using predefined test cases. We assumed it would feel more systematic but less creative.

2 # High-level description of the exploratory testing plan


For exploratory testing, we adopted a **user-centric approach**, simulating real-world scenarios without predefined scripts. Key areas tested included:
- **Transactions**: Deposits, withdrawals, and transfers.
- **Edge Cases**: Zero-dollar transfers, invalid PIN entries, and boundary amounts.
- **UI/UX**: Menu navigation, error messages, and receipt accuracy.

We prioritized **risk-prone features**, such as fund transfers and balance calculations, and allowed intuition to guide test execution. For example, entering a 23-digit PIN revealed a system crash, and testing $0.00 transfers exposed balance deduction flaws.


3 # Comparison of exploratory and manual functional testing

| **Aspect**               | **Exploratory Testing**                                  | **Manual Functional Testing**                |  
|--------------------------|---------------------------------------------------------|----------------------------------------------|  
| **Mindset**              | “What happens if I…?” – Curiosity-driven, like solving a puzzle. | “Does it do what it’s supposed to?” – Verifying a checklist. |  
| **Bug Hunting**          | Found hidden flaws (e.g., transferring $0.00 *subtracted* money!). | Confirmed basics (e.g., preset withdrawals dispensed cash). |  
| **Creativity**           | Led to oddball tests (e.g., 23-digit PIN crashes the system). | Followed scripts (e.g., testing $20 withdrawals). |  
| **Surprises**            | Exposed UI quirks (e.g., Money Market balance showing “Unknown Error”). | Missed edge cases (e.g., missing Savings account option). |  
| **Speed vs. Precision**  | Fast for uncovering critical issues (e.g., balance deductions). | Slower but ensured no step was overlooked. |  
| **Documentation**        | Notes were messy but real-time (e.g., “Receipt flips transfer direction!”). | Neat logs (e.g., “Step 3: Withdraw $60 – PASS”). |  
| **User Experience**      | Mimicked confused users (e.g., entering gibberish PINs). | Mimicked ideal users (e.g., valid transactions only). |  
| **Team Dynamics**        | Felt collaborative – “Hey, try transferring $0.00!” | Felt structured – “You test Option 3; I’ll test Option 4.” |  

#### Why Both Matter:  
- **Exploratory testing** was our “aha!” moment – like realizing the ATM charged a secret $0.50 fee for *doing nothing*.  
- **Manual testing** was our safety net – ensuring the basics (e.g., withdrawing $20) didn’t break.  
- Together, they painted a full picture: the ATM worked… until it *didn’t*. 

During an Exploratory testing – we tried weird stuff just to see what would break. Like, who knew transferring $0.00 would *steal* money from your account? Manual testing was more like following predefined steps like: “Step 1: Put in card. Step 2: Take out $20.” It worked, but it didn’t catch the sneaky bugs.  

Together, they were like a superhero duo. Exploratory testing found the “Why is my balance $500 instead of $5000?!” mess. Manual testing made sure the basics (like withdrawing cash) didn’t blow up. Not perfect, but at least we knew the ATM could handle real people.   

4 # Notes and discussion of the peer reviews of defect reports

We conducted **peer reviews** to ensure defect reports were clear, reproducible, and prioritized. Key observations:
- **Clarity**: Reports initially lacked screenshots for UI errors (e.g., "Unknown Error" screen). We added attachments to improve reproducibility.
- **Severity**: Critical bugs (e.g., balance deductions) were prioritized over cosmetic issues (e.g., missing "$" signs).
- **Feedback**: Falmata suggested refining steps to reproduce for complex defects (e.g., divergent Git branches).

5 # How the pair testing was managed and team work/effort was divided 

We divided tasks based on expertise and rotated roles:
- **Henok**: Focused on transaction logic (deposits, transfers) and defect documentation.
- **Falmata**: Tested UI/UX flows (balance inquiries, error handling) and managed Jira tickets.

For pair testing, we used a **"driver-Reporter"** model:
- The **driver** executed tests.
- The **Reporter** observed, took notes, and suggested edge cases.

Daily sync-ups ensured alignment, and version control (Git) was used to track code changes.

6 # Difficulties encountered, challenges overcome, and lessons learned

### Challenges:
- **Divergent Git Branches**: Caused push failures; resolved with `git rebase`.
- **Inconsistent Defect Behavior**: Some bugs behaved differently across versions (e.g., $0.50 deduction in V1.0 vs. no deduction in V1.1).
- **Time Constraints**: Balancing thorough testing with lab deadlines, and not
knowing acutally how many bugs are in the program.

### Lessons Learned:
- **Test Early, Test Often**: Catching defects early reduces downstream risks.
- **Document Everything**: Clear steps-to-reproduce are critical for developers.
- **Communicate Clearly**: Regular team check-ins prevented duplicated efforts.


7 # Comments/feedback on the lab and lab document itself

The assignment description is overly lengthy, making it harder to quickly grasp the key requirements. A more concise version would improve clarity and readability.
